{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST database\n",
    "\n",
    "The MNIST (Modified National Institute of Standards and Technology) database is a public dataset of handwritten digits:\n",
    "\n",
    "- There are 60,000 training images and 10,000 testing images in the dataset.\n",
    "- Digits are normalized to fit in a 28x28 pixel bounding box.\n",
    "- Each grayscale image pixel is represented by an 8-bit integer value between 0 (black) and 255 (white).\n",
    "- Each image is annotated with the digit shown in the image. This annotation / label is called ground truth.\n",
    "\n",
    "The figure below shows the first 20 training images as well as the respective ground truth labels.\n",
    "\n",
    "![image.png](figures/mnist.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to load and visualize the data\n",
    "\n",
    "def load_data():\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "    # normalize images\n",
    "    x_train = x_train / 255.0\n",
    "    x_test = x_test / 255.0\n",
    "\n",
    "    # one-hot encode labels\n",
    "    y_train = tf.keras.utils.to_categorical(y_train)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "def visualize_mnist(n):\n",
    "    \"\"\"For each digit, show the first n images from the training dataset.\n",
    "    \"\"\"\n",
    "    assert n > 0\n",
    "    (x, y), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "    cols = 10\n",
    "    rows = n\n",
    "\n",
    "    plt.figure(figsize=(12, rows*1.5))\n",
    "    for i in range(10):\n",
    "        idx = y == i\n",
    "        for j in range(n):\n",
    "            plt.subplot(rows, cols, i+10*j+1)\n",
    "            plt.imshow(x[idx][j], cmap='gray')\n",
    "            plt.xlabel(i)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and preprocess data\n",
    "(x_train, y_train), (x_test, y_test) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the first 50 images from the training set\n",
    "visualize_mnist(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training\n",
    "\n",
    "The neural nets (`keras.Model` instances) are trained on 90% of the training data.\n",
    "The remaining 10%, i.e., 6,000 pairs of images and labels, are reserved for validation.\n",
    "\n",
    "During training, we monitor both the loss of the training and the validation split over the training iterations (epochs).\n",
    "If the validation loss does not decrease for more than three consecutive epochs, we end the training (*early stopping*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, patience=3):\n",
    "    \"\"\"Train the model as long as the validation loss is decreasing\n",
    "    and return the history of the training process.\n",
    "    \"\"\"\n",
    "\n",
    "    # stops training when a monitored metric (here validation loss) has stopped improving\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=patience)\n",
    "\n",
    "    hist = model.fit(\n",
    "        x_train, \n",
    "        y_train,\n",
    "        shuffle=True,\n",
    "        epochs=100,\n",
    "        batch_size=512, \n",
    "        validation_split=0.1, \n",
    "        callbacks=[early_stopping],\n",
    "    )\n",
    "    return hist\n",
    "\n",
    "def plot_history(hist):\n",
    "    \"\"\"Plot the training and validation loss and accuracy over the epochs.\"\"\"\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    # plot the training and validation accuracy in left subplot\n",
    "    plt.plot(hist.history['accuracy'], label='training')\n",
    "    plt.plot(hist.history['val_accuracy'], label='validation')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy [%]')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    # plot the training and validation loss on logarithmic scale in right subplot\n",
    "    plt.plot(hist.history['loss'], label='training')\n",
    "    plt.plot(hist.history['val_loss'], label='validation')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss [-]')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward Network for Classification\n",
    "\n",
    "We define the following fully-connected feedforward network architecture to classify the handwritten digit images:\n",
    "\n",
    "1. Input Layer: 2D images represented as 28x28 matrix\n",
    "2. Flatten Layer: Transforms the 2D images (28x28 pixels) into a 1D column vector of size 784=28^2\n",
    "3. Hidden fully-connected layer with 128 neurons and ReLU activation\n",
    "4. Output layer, fully connected with 10 neurons and Softmax activation. Models a probability distribution over the available digits 0-9.\n",
    "\n",
    "Since our goal is multi-class classification, we choose the categorical cross-entropy as our loss function.\n",
    "Moreover, we instruct the model to monitor the classification accuracy as an additional metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "simple_ffn = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(units=128, activation='relu'),\n",
    "    # output layer: number of units is the number of classes\n",
    "    tf.keras.layers.Dense(units=10, activation='softmax')\n",
    "])\n",
    "simple_ffn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_ffn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_ffn_hist = train_model(simple_ffn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(simple_ffn_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparamters\n",
    "\n",
    "Beside the trainable parameters (i.e. weights and biases in the fully-connected layers) there are a number of hyperparameters which affect the overall architecture, size and training scheme of the neural network.\n",
    "While trainable parameters are optimized with gradient decent to minimize the specified loss function, hyperparameters are design choices made by the programmer.\n",
    "However, the choice of hyperparameters has a significant impact on the model's performance and thus needs to be optimized as well.\n",
    "Such a hyperparameter optimization is steered by the model's performance on validation data and can be done manually or automatically.\n",
    "\n",
    "Typical hyperparameters are:\n",
    "\n",
    "- depth: number of hidden layers\n",
    "- width: number of units (neurons) per (hidden) layer\n",
    "- type of activation function\n",
    "- optimizer\n",
    "- regularization\n",
    "- number of iterations (epochs): we have already optimized this parameter with early stopping\n",
    "\n",
    "In the following, we will manually investigate the impact of the depth and width on the classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep feedforward neural network\n",
    "deep_ffn = models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(units=128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.003)),\n",
    "    tf.keras.layers.Dense(units=128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.003)),\n",
    "    tf.keras.layers.Dense(units=128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.003)),\n",
    "    # output layer: number of units is the number of classes\n",
    "    tf.keras.layers.Dense(units=10, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(0.003))\n",
    "])\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.3)\n",
    "deep_ffn.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# wide feedforward neural network\n",
    "wide_ffn = models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(units=512, activation='relu'),\n",
    "    # output layer: number of units is the number of classes\n",
    "    tf.keras.layers.Dense(units=10, activation='softmax')\n",
    "])\n",
    "wide_ffn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_ffn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_hist = train_model(deep_ffn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(deep_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_ffn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_hist = train_model(wide_ffn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(wide_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe (informally):\n",
    " - neither the wide nor the deep network improve the classification performance significantly\n",
    " - the wide network has significantly more parameters\n",
    " - the deep network is slower than the wide network (only true on my machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "\n",
    "Next we implement a simple CNN by replacing the hidden Dense layer with a 2D convolutional layer.\n",
    "The model's architecture looks as follows:\n",
    "\n",
    "1. Input Layer: 2D images represented as 28x28 matrix\n",
    "2. 2D Convolutional Layer with 32 independent 5x5 pixel filters\n",
    "3. 2D MaxPooling Layer: summarizes pixel of a 2x2 neighborhood with their maximum. This aims to emphasize the most dominant feature.\n",
    "4. Flatten Layer: Transforms the convolution output (12x12x32 tensor) into a 1D column vector.\n",
    "5. Output layer, fully connected with 10 neurons and Softmax activation. Models a probability distribution over the available digits 0-9.\n",
    "\n",
    "When applied on image data, CNNs have a various number of benefits over FFN:\n",
    "- they preserve the spatial relations in the image\n",
    "- due to parameter sharing of the kernels, patterns are detected regardless of the position in the image (translational invariance)\n",
    "- they use fewer parameters than fully-connected layers and are less prone to overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_cnn = models.Sequential([\n",
    "    layers.Conv2D(32, (5, 5), activation=\"relu\", input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(units=10, activation=\"softmax\"),\n",
    "])\n",
    "simple_cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_cnn_hist = train_model(simple_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(simple_cnn_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe overfitting after roughly 7 epochs of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-Performance CNN\n",
    "\n",
    "We implement a CNN with Keras according to the architecture proposed by Brendan Artley: https://medium.com/@BrendanArtley/mnist-keras-simple-cnn-99-6-731b624aee7f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_cnn = models.Sequential([\n",
    "    layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding=\"same\", input_shape=(28, 28, 1)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding=\"same\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding=\"same\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding=\"same\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding=\"same\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding=\"same\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "hp_cnn.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_cnn_hist = train_model(hp_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(hp_cnn_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods to evaluate the model and visualize misclassifications\n",
    "\n",
    "def evaluate_model(model):\n",
    "    \"\"\"Evaluate the model on the test set.\"\"\"\n",
    "\n",
    "    loss, accuracy = model.evaluate(x_test, y_test, batch_size=1024)\n",
    "    print(f\"Test Loss: {loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "    # Return the loss and accuracy\n",
    "    return loss, accuracy\n",
    "\n",
    "def find_misclassifications(model, n=20):\n",
    "    predictions = model.predict(x_test, batch_size=1024)\n",
    "    y_true = y_test.argmax(axis=1)\n",
    "    y_pred = predictions.argmax(axis=1)\n",
    "\n",
    "    # Get the indices corresponding to misclassifications\n",
    "    misclassified_indices = np.where(y_pred != y_true)[0]\n",
    "\n",
    "    for i, idx in enumerate(misclassified_indices):\n",
    "        if i < n:\n",
    "            visualize_miss(x_test[idx], y_true[idx], predictions[idx])\n",
    "        else:\n",
    "            break\n",
    "\n",
    "\n",
    "def visualize_miss(x, y, y_pred):\n",
    "    \"\"\"Visualizes a single misclassification.\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(12,1.5))\n",
    "\n",
    "    # show the image and the predicted and true label in the lefthand subplot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(x, cmap='gray')\n",
    "    plt.xlabel(f'Predicted: {np.argmax(y_pred)} Expected: {y}')\n",
    "\n",
    "    # show the probabilities bar chart in the righthand subplot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(range(10), y_pred * 100)\n",
    "    plt.xticks(range(10))\n",
    "    plt.xlabel('Digit')\n",
    "    plt.ylabel('Probability [%]')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = evaluate_model(simple_ffn)\n",
    "_ = evaluate_model(simple_cnn)\n",
    "_ = evaluate_model(hp_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_misclassifications(hp_cnn, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m1tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
