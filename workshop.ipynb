{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST database\n",
    "\n",
    "The MNIST (Modified National Institute of Standards and Technology) database is a public dataset of handwritten digits:\n",
    "\n",
    "- There are 60,000 training images and 10,000 testing images in the dataset.\n",
    "- Digits are normalized to fit in a 28x28 pixel bounding box.\n",
    "- Each grayscale image pixel is represented by an 8-bit integer value between 0 (black) and 255 (white).\n",
    "- Each image is annotated with the digit shown in the image. This annotation / label is called ground truth.\n",
    "\n",
    "The figure below shows the first 20 training images as well as the respective ground truth labels.\n",
    "\n",
    "![image.png](figures/mnist.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to load and visualize the data\n",
    "\n",
    "def load_data():\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "    # normalize images\n",
    "    x_train = x_train / 255.0\n",
    "    x_test = x_test / 255.0\n",
    "\n",
    "    # one-hot encode labels\n",
    "    y_train = tf.keras.utils.to_categorical(y_train)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "def visualize_mnist(n):\n",
    "    \"\"\"For each digit, show the first n images from the training dataset.\n",
    "    \"\"\"\n",
    "    assert n > 0\n",
    "    (x, y), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "    cols = 10\n",
    "    rows = n\n",
    "\n",
    "    plt.figure(figsize=(16, rows*2))\n",
    "    for i in range(10):\n",
    "        idx = y == i\n",
    "        for j in range(3):\n",
    "            plt.subplot(rows, cols, i+10*j+1)\n",
    "            plt.imshow(x[idx][j], cmap='gray')\n",
    "            plt.xlabel(i)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and preprocess data\n",
    "(x_train, y_train), (x_test, y_test) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the first 30 images from the training set\n",
    "visualize_mnist(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training\n",
    "\n",
    "The neural nets (`keras.Model` instances) are trained on 90% of the training data.\n",
    "The remaining 10%, i.e., 6,000 pairs of images and labels, are reserved for validation.\n",
    "\n",
    "During training, we monitor both the loss of the training and the validation split over the training iterations (epochs).\n",
    "If the validation loss does not decrease for more than three consecutive epochs, we end the training (*early stopping*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    \"\"\"Train the model as long as the validation loss is decreasing\n",
    "    and return the history of the training process.\n",
    "    \"\"\"\n",
    "\n",
    "    # stops training when a monitored metric (here validation loss) has stopped improving\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "    hist = model.fit(\n",
    "        x_train, \n",
    "        y_train,\n",
    "        shuffle=True,\n",
    "        epochs=100,\n",
    "        batch_size=512, \n",
    "        validation_split=0.1, \n",
    "        callbacks=[early_stopping],\n",
    "    )\n",
    "    return hist\n",
    "\n",
    "def plot_history(hist):\n",
    "    \"\"\"Plot the training and validation loss and accuracy over the epochs.\"\"\"\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    # plot the training and validation accuracy in left subplot\n",
    "    plt.plot(hist.history['accuracy'], label='training')\n",
    "    plt.plot(hist.history['val_accuracy']*100, label='validation')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy [%]')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    # plot the training and validation loss on logarithmic scale in right subplot\n",
    "    plt.plot(hist.history['loss'], label='training')\n",
    "    plt.plot(hist.history['val_loss'], label='validation')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss [-]')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward Network for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "simple_ffn = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(units=128, activation='relu'),\n",
    "    # output layer: number of units is the number of classes\n",
    "    tf.keras.layers.Dense(units=10, activation='softmax')\n",
    "])\n",
    "simple_ffn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_ffn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_ffn_hist = train_model(simple_ffn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep feedforward neural network\n",
    "deep_ffn = models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(units=128, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=128, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=128, activation='relu'),\n",
    "    # output layer: number of units is the number of classes\n",
    "    tf.keras.layers.Dense(units=10, activation='softmax')\n",
    "])\n",
    "deep_ffn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# wide feedforward neural network\n",
    "wide_ffn = models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(units=512, activation='relu'),\n",
    "    # output layer: number of units is the number of classes\n",
    "    tf.keras.layers.Dense(units=10, activation='softmax')\n",
    "])\n",
    "wide_ffn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_ffn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(deep_ffn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_ffn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(wide_ffn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularized deep feedforward neural network\n",
    "reg_ffn = models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(units=512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(units=128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    # output layer: number of units is the number of classes\n",
    "    tf.keras.layers.Dense(units=10, activation='softmax')\n",
    "])\n",
    "reg_ffn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(reg_ffn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_cnn = models.Sequential([\n",
    "    layers.Conv2D(32, (5, 5), activation=\"relu\", input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(units=10, activation=\"softmax\"),\n",
    "])\n",
    "simple_cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(simple_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-Performance CNN\n",
    "\n",
    "We implement a CNN with Keras according to the architecture proposed by Brendan Artley: https://medium.com/@BrendanArtley/mnist-keras-simple-cnn-99-6-731b624aee7f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_cnn = models.Sequential([\n",
    "    layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding=\"same\", input_shape=(28, 28, 1)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding=\"same\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding=\"same\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding=\"same\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding=\"same\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding=\"same\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPool2D(pool_size=(2, 2), strides=2),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "hp_cnn.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_cnn_hist = train_model(hp_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(hp_cnn_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods to evaluate the model and visualize misclassifications\n",
    "\n",
    "def evaluate_model(model):\n",
    "    \"\"\"Evaluate the model on the test set.\"\"\"\n",
    "\n",
    "    loss, accuracy = model.evaluate(x_test, y_test, batch_size=1024)\n",
    "    print(f\"Test Loss: {loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "    # Return the loss and accuracy\n",
    "    return loss, accuracy\n",
    "\n",
    "def find_misclassifications(model, n=20):\n",
    "    predictions = model.predict(x_test, batch_size=1024)\n",
    "    y_true = y_test.argmax(axis=1)\n",
    "    y_pred = predictions.argmax(axis=1)\n",
    "\n",
    "    # Get the indices corresponding to misclassifications\n",
    "    misclassified_indices = np.where(y_pred != y_true)[0]\n",
    "\n",
    "    for i, idx in enumerate(misclassified_indices):\n",
    "        if i < n:\n",
    "            visualize_miss(x_test[idx], y_true[idx], predictions[idx])\n",
    "        else:\n",
    "            break\n",
    "\n",
    "\n",
    "def visualize_miss(x, y, y_pred):\n",
    "    \"\"\"Visualizes a single misclassification.\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(12,1.5))\n",
    "\n",
    "    # show the image and the predicted and true label in the lefthand subplot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(x, cmap='gray')\n",
    "    plt.xlabel(f'Predicted: {np.argmax(y_pred)} Expected: {y}')\n",
    "\n",
    "    # show the probabilities bar chart in the righthand subplot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(range(10), y_pred * 100)\n",
    "    plt.xticks(range(10))\n",
    "    plt.xlabel('Digit')\n",
    "    plt.ylabel('Probability [%]')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = evaluate_model(simple_cnn)\n",
    "_ = evaluate_model(hp_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_misclassifications(hp_cnn, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_cnn.predict(x_test[0:1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m1tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
